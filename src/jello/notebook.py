"""
Jupyter notebook launcher for GPT-OSS model experimentation on Runpod GPU.
"""
from __future__ import annotations
import json
import os
import secrets
import subprocess
import sys
from pathlib import Path
from typing import Optional

import typer
from rich import print


app = typer.Typer(add_completion=False)


def _run(cmd: list[str]) -> None:
    subprocess.run(cmd, check=True)


def _maybe_register_kernel(kernel_name: str = "jello", display_name: str = "Python (jello)") -> None:
    """
    Register the *current interpreter* as a Jupyter kernel so notebooks see the same env (uv/.venv).
    Safe to run repeatedly.
    """
    try:
        _run([sys.executable, "-m", "ipykernel", "install", "--user", f"--name={kernel_name}", f"--display-name={display_name}"])
        print(f"[green]✓ Registered Jupyter kernel:[/green] {display_name}")
    except Exception as e:
        print(f"[yellow]! Skipped kernel registration[/yellow]: {e}")


def _write_jupyter_config(root_dir: Path, allow_origin_pat: str = r"^https?://.*runpod\.net$") -> Path:
    """
    Write a proxy-friendly jupyter_server_config.py that:
    - binds to 0.0.0.0:8888
    - disables port retries
    - relaxes XSRF/CORS for RunPod proxy
    - points root_dir to the chosen notebook directory
    """
    cfg_dir = Path.home() / ".jupyter"
    cfg_dir.mkdir(parents=True, exist_ok=True)
    cfg_path = cfg_dir / "jupyter_server_config.py"
    cfg = f"""# Auto-generated by launcher
c = get_config()
c.ServerApp.ip = '0.0.0.0'
c.ServerApp.port = 8888
c.ServerApp.port_retries = 0
c.ServerApp.allow_remote_access = True
c.ServerApp.trust_xheaders = True
c.ServerApp.use_redirect_file = False
c.ServerApp.root_dir = r'{root_dir.as_posix()}'
c.ServerApp.allow_origin_pat = r'{allow_origin_pat}'
# Relax XSRF because many reverse proxies (incl. RunPod) rewrite headers
c.ServerApp.disable_check_xsrf = True
"""
    cfg_path.write_text(cfg, encoding="utf-8")
    return cfg_path


def create_starter_notebook(
    notebook_path: Path,
    model_name: str = "openai/gpt-oss-20b",
    dtype: str = "auto",
    device_map: str = "auto",
) -> None:
    """Create a Jupyter notebook with pre-configured model loading."""
    # NOTE: Keep examples safe; avoid actionable harmful prompts.
    notebook_content = {
        "cells": [
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "# GPT-OSS Model Experimentation\n",
                    f"Pre-configured notebook for `{model_name}` on RunPod GPU.\n\n",
                    "## Setup\n",
                    f"- Model: `{model_name}`\n",
                    f"- Device mapping: `{device_map}`\n",
                    f"- Dtype: `{dtype}`\n",
                ],
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": [
                    "# Import required libraries\n",
                    "import torch\n",
                    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                    "from jello.llm.runner import load_gptoss, GPTOSS\n",
                    "from jello.analysis.introspect import (\n",
                    "    summarize_router_logits,\n",
                    "    layer_activation_norms,\n",
                    "    attention_head_entropies,\n",
                    ")\n",
                    "from jello.probes.attention import collect_attentions, head_distraction_index\n",
                    "import matplotlib.pyplot as plt\n",
                    "import numpy as np\n",
                    "\n",
                    "print(f\"PyTorch version: {torch.__version__}\")\n",
                    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                    "if torch.cuda.is_available():\n",
                    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
                    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
                ],
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": [
                    "# Load GPT-OSS model\n",
                    f"model_name = \"{model_name}\"\n",
                    "llm = load_gptoss(\n",
                    "    model_name,\n",
                    f"    dtype=\"{dtype}\",\n",
                    f"    device_map=\"{device_map}\",\n",
                    ")\n",
                    "print(f\"✅ Loaded {model_name}\")\n",
                    "try:\n",
                    "    print(f\"Device: {llm.model.device}\")\n",
                    "    print(f\"Model dtype: {llm.model.dtype}\")\n",
                    "except Exception: pass\n",
                ],
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": [
                    "# Quick chat test\n",
                    "messages = [{\"role\": \"user\", \"content\": \"Hello! Give a one-sentence fun fact.\"}]\n",
                    "response = llm.chat(messages, max_new_tokens=60, temperature=0.7)\n",
                    "print(\"🤖 Response:\\n\", response)\n",
                ],
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "## Model Internals Analysis\n",
                    "Capture and analyze attention patterns, hidden states, and MoE routing.\n",
                ],
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": [
                    "# Analyze model internals for a benign prompt\n",
                    "prompt = \"Summarize why mixture-of-experts can speed up inference.\"\n",
                    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
                    "outputs = llm.forward_inspect(\n",
                    "    messages,\n",
                    "    output_hidden_states=True,\n",
                    "    output_attentions=True,\n",
                    "    output_router_logits=True,\n",
                    ")\n",
                    "print(\"📊 Captured model internals:\")\n",
                    "for key, value in outputs.items():\n",
                    "    try:\n",
                    "        if hasattr(value, 'shape'):\n",
                    "            print(f\"  {key}: {value.shape}\")\n",
                    "        elif isinstance(value, (list, tuple)) and len(value) > 0 and hasattr(value[0], 'shape'):\n",
                    "            print(f\"  {key}: {len(value)} layers, each {value[0].shape}\")\n",
                    "        else:\n",
                    "            print(f\"  {key}: {type(value)}\")\n",
                    "    except Exception:\n",
                    "        print(f\"  {key}: (unprintable shape)\")\n",
                ],
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": [
                    "# Activation norms across layers\n",
                    "if \"hidden_states\" in outputs and outputs[\"hidden_states\"] is not None:\n",
                    "    norms = layer_activation_norms(outputs[\"hidden_states\"])\n",
                    "    plt.figure(figsize=(12, 6))\n",
                    "    plt.imshow(norms.cpu().numpy(), aspect='auto')\n",
                    "    plt.colorbar(label='L2 Norm')\n",
                    "    plt.xlabel('Token Position')\n",
                    "    plt.ylabel('Layer')\n",
                    "    plt.title('Activation Norms Across Layers')\n",
                    "    plt.show()\n",
                    "    print(f\"Activation norms shape: {getattr(norms, 'shape', None)}\")\n",
                ],
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": [
                    "# MoE routing analysis (if available)\n",
                    "router_logits = outputs.get(\"router_logits\")\n",
                    "if router_logits:\n",
                    "    moe_summaries = summarize_router_logits(router_logits, top_k=3)\n",
                    "    print(f\"\\n🔀 MoE: {len(moe_summaries)} layers with expert routing\")\n",
                    "    for summary in moe_summaries[:3]:\n",
                    "        expert_ids = summary.topk_expert_ids[0]\n",
                    "        expert_probs = summary.topk_expert_probs[0]\n",
                    "        print(f\"Layer {summary.layer_index}: top experts (first 5 tokens) -> {expert_ids[:5].tolist()} | probs -> {expert_probs[:5].tolist()}\")\n",
                    "else:\n",
                    "    print(\"No MoE routing detected (model might not be MoE)\")\n",
                ],
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": [
                    "# Attention head entropies\n",
                    "if \"attentions\" in outputs and outputs[\"attentions\"] is not None:\n",
                    "    entropies = attention_head_entropies(outputs[\"attentions\"])\n",
                    "    avg_entropies = entropies.mean(dim=-1).cpu().numpy()\n",
                    "    import matplotlib.pyplot as plt\n",
                    "    plt.figure(figsize=(12, 8))\n",
                    "    plt.imshow(avg_entropies, aspect='auto')\n",
                    "    plt.colorbar(label='Attention Entropy')\n",
                    "    plt.xlabel('Attention Head')\n",
                    "    plt.ylabel('Layer')\n",
                    "    plt.title('Average Attention Entropy per Head')\n",
                    "    plt.show()\n",
                    "    print(f\"Attention entropies shape: {entropies.shape}\")\n",
                ],
            },
        ],
        "metadata": {
            "kernelspec": {
                "display_name": "Python (jello)",
                "language": "python",
                "name": "jello",
            },
            "language_info": {
                "name": "python",
                "version": ".".join(map(str, sys.version_info[:3])),
            },
        },
        "nbformat": 4,
        "nbformat_minor": 4,
    }

    notebook_path.parent.mkdir(parents=True, exist_ok=True)
    with open(notebook_path, "w") as f:
        json.dump(notebook_content, f, indent=2)


@app.command()
def notebook(
    port: int = typer.Option(8888, help="HTTP port"),
    host: str = typer.Option("0.0.0.0", help="Bind IP"),
    notebook_dir: Optional[Path] = typer.Option(
        Path("/workspace/notebooks"), help="Notebook root dir (created if missing)"
    ),
    create_starter: bool = typer.Option(True, help="Create a starter notebook if missing"),
    model_name: str = typer.Option("openai/gpt-oss-20b", help="Default model in starter NB"),
    kill_stragglers: bool = typer.Option(True, help="Kill any existing Jupyter before starting"),
) -> None:
    """Launch JupyterLab with proxy-friendly config and the current venv as a kernel."""
    # Ensure notebook dir exists and is writable
    if notebook_dir is None:
        notebook_dir = Path("/workspace/notebooks")
    notebook_dir = notebook_dir.resolve()
    notebook_dir.mkdir(parents=True, exist_ok=True)

    # Register kernel from this interpreter (uv/.venv) so imports match your env
    _maybe_register_kernel()

    # Optional starter
    if create_starter:
        starter_path = notebook_dir / "gpt_oss_starter.ipynb"
        if not starter_path.exists():
            print(f"🚀 Creating starter notebook: {starter_path}")
            create_starter_notebook(starter_path, model_name)
        else:
            print(f"📓 Starter notebook already exists: {starter_path}")

    # Write a proxy-friendly config (RunPod)
    cfg_path = _write_jupyter_config(notebook_dir)

    # Token handling: keep ON by default; disable only if env says so
    token_env = os.environ.get("JUPYTER_TOKEN")
    no_token = os.environ.get("JUPYTER_NO_TOKEN", "").lower() in {"1", "true", "yes"}
    token = ""
    token_flag = []
    if no_token:
        print("[yellow]⚠ Disabling Jupyter token (only safe behind private RunPod link).[/yellow]")
        token_flag = ["--ServerApp.token=", "--ServerApp.password="]
    else:
        token = token_env or secrets.token_urlsafe(16)
        token_flag = [f"--ServerApp.token={token}"]

    # Kill any stray Jupyter so 8888 is free
    if kill_stragglers:
        try:
            subprocess.run(["pkill", "-f", "jupyter.*lab"], check=False)
        except Exception:
            pass

    # Launch Jupyter with same interpreter as this script (keeps uv/.venv)
    cmd = [
        sys.executable, "-m", "jupyter", "lab",
        "--ip", host,
        "--port", str(port),
        "--no-browser",
        "--allow-root",  # Needed in containers
        "--ServerApp.port_retries=0",
        "--ServerApp.allow_remote_access=True",
        "--ServerApp.trust_xheaders=True",
        "--ServerApp.use_redirect_file=False",
        f"--notebook-dir={notebook_dir.as_posix()}",
        # allow RunPod proxy origins
        "--ServerApp.allow_origin_pat=^https?://.*runpod\\.net$",
        # relax XSRF due to proxy header rewriting
        "--ServerApp.disable_check_xsrf=True",
        *token_flag,
    ]

    print(f"🔬 Launching JupyterLab on {host}:{port}")
    print(f"📂 Notebook directory: {notebook_dir}")
    if token:
        print(f"🔑 Token: {token}")
    print(f"🛠  Config: {cfg_path}")

    # Helpful hint for RunPod proxy users
    print(f"🌐 If using RunPod proxy, open the provided https://…proxy.runpod.net:{port} URL.")

    try:
        subprocess.run(cmd, check=True)
    except subprocess.CalledProcessError as e:
        print(f"❌ Failed to launch Jupyter: {e}")
        sys.exit(1)
    except KeyboardInterrupt:
        print("\n🛑 JupyterLab stopped")


if __name__ == "__main__":
    app()
